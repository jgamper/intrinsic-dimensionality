{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_fast_walsh_hadamard(x, axis, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute Fast Walsh-Hadamard transform in numpy.\n",
    "    Args:\n",
    "        x: tensor of shape (a0, a1, ... aN, L, b0, b1, ..., bN).\n",
    "            L must be a power of two.\n",
    "        axis: the \"L\" axis above, aka the axis over which to do the\n",
    "            Hadamard transform. All other dimensions are left alone;\n",
    "            data on those dimension do not interact.\n",
    "        normalize: Whether to normalize the results such that applying\n",
    "            the transform twice returns to the original input\n",
    "            value. If True, return values are floats even if input was\n",
    "            int.\n",
    "    Returns:\n",
    "        ret: transformed tensor with same shape as x\n",
    "    \"\"\"\n",
    "\n",
    "    orig_shape = x.shape\n",
    "    assert axis >= 0 and axis < len(orig_shape), (\n",
    "        'For a vector of shape %s, axis must be in [0, %d] but it is %d'\n",
    "        % (orig_shape, len(orig_shape) - 1, axis))\n",
    "    h_dim = orig_shape[axis]\n",
    "    h_dim_exp = int(round(np.log(h_dim) / np.log(2)))\n",
    "    assert h_dim == 2 ** h_dim_exp, (\n",
    "        'hadamard can only be computed over axis with size that is a power of two, but'\n",
    "        ' chosen axis %d has size %d' % (axis, h_dim))\n",
    "    working_shape_pre = [int(np.prod(orig_shape[:axis]))]     # prod of empty array is 1 :)\n",
    "    working_shape_post = [int(np.prod(orig_shape[axis+1:]))]  # prod of empty array is 1 :)\n",
    "    working_shape_mid = [2] * h_dim_exp\n",
    "    working_shape = working_shape_pre + working_shape_mid + working_shape_post\n",
    "    #print 'working_shape is', working_shape\n",
    "    ret = x.reshape(working_shape)\n",
    "\n",
    "    for ii in range(h_dim_exp):\n",
    "        dim = ii + 1\n",
    "        arrs = np.split(ret, 2, axis=dim)\n",
    "        assert len(arrs) == 2\n",
    "        inter = np.concatenate((arrs[0] + arrs[1], arrs[0] - arrs[1]), axis=dim)\n",
    "        ret = inter\n",
    "\n",
    "    if normalize:\n",
    "        ret = ret / np.sqrt(float(h_dim))\n",
    "\n",
    "    ret = ret.reshape(orig_shape)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def fast_walsh_hadamard_torched(x, axis, normalize=False):\n",
    "    orig_shape = x.size()\n",
    "    assert axis >= 0 and axis < len(orig_shape), (\n",
    "    'For a vector of shape %s, axis must be in [0, %d] but it is %d'\n",
    "    % (orig_shape, len(orig_shape) - 1, axis))\n",
    "    h_dim = orig_shape[axis]\n",
    "    h_dim_exp = int(round(np.log(h_dim) / np.log(2)))\n",
    "    assert h_dim == 2 ** h_dim_exp, (\n",
    "    'hadamard can only be computed over axis with size that is a power of two, but'\n",
    "    ' chosen axis %d has size %d' % (axis, h_dim))\n",
    "    \n",
    "    working_shape_pre = [int(np.prod(orig_shape[:axis]))]     # prod of empty array is 1 :)\n",
    "    working_shape_post = [int(np.prod(orig_shape[axis+1:]))]  # prod of empty array is 1 :)\n",
    "    working_shape_mid = [2] * h_dim_exp\n",
    "    working_shape = working_shape_pre + working_shape_mid + working_shape_post\n",
    "    \n",
    "    ret = x.view(working_shape)\n",
    "    \n",
    "    for ii in range(h_dim_exp):\n",
    "        dim = ii + 1\n",
    "        arrs = torch.chunk(ret, 2, dim=dim)\n",
    "        assert len(arrs) == 2\n",
    "        ret = torch.cat((arrs[0] + arrs[1], arrs[0] - arrs[1]), axis=dim)\n",
    "    \n",
    "    if normalize:\n",
    "        ret = ret / torch.sqrt(float(h_dim))\n",
    "    \n",
    "    ret = ret.view(orig_shape)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def fastfood_torched(x, DD):\n",
    "\n",
    "    ll = int(np.ceil(np.log(DD) / np.log(2)))\n",
    "    LL = 2 ** ll\n",
    "\n",
    "    BB = torch.FloatTensor(LL).uniform_(0, 2).type(torch.LongTensor)\n",
    "    BB = (BB * 2 - 1).type(torch.FloatTensor)\n",
    "\n",
    "    Pi = torch.LongTensor(np.random.permutation(LL))\n",
    "\n",
    "    GG = torch.FloatTensor(LL,).normal_()\n",
    "\n",
    "    divisor = torch.sqrt(LL * torch.sum(torch.pow(GG, 2)))\n",
    "\n",
    "    fastfood_vars = [BB, Pi, GG, divisor]\n",
    "\n",
    "    dd_pad = F.pad(x, pad=(0, LL - dd), value=0, mode='constant')\n",
    "\n",
    "    mul_1 = torch.mul(BB, dd_pad)\n",
    "\n",
    "    mul_2 = fast_walsh_hadamard_torched(mul_1, 0, normalize=False)\n",
    "\n",
    "    mul_3 = mul_2[Pi]\n",
    "\n",
    "    mul_4 = torch.mul(mul_3, GG)\n",
    "\n",
    "    mul_5 = fast_walsh_hadamard_torched(mul_4, 0, normalize=False)\n",
    "\n",
    "    ret = torch.div(mul_5[:DD], divisor*np.sqrt(float(DD) / LL))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastfood below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = 4\n",
    "DD = 20\n",
    "x = np.array([ 1.6817038, 0.94111705, -0.4930046, 0.9057069 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "print(x.size())\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6817,  0.9411, -0.4930,  0.9057])\n",
      "torch.Size([20])\n",
      "tensor([ 0.0758, -0.8598,  0.6764,  0.3636, -0.4214, -0.0456, -0.3712,  0.2620,\n",
      "         0.1477, -0.3099,  0.4032, -0.0119, -0.6375, -0.0612, -0.3593,  0.6843,\n",
      "        -0.0042,  0.1823,  0.6003, -0.2568])\n"
     ]
    }
   ],
   "source": [
    "X = fastfood_torched(x, DD)\n",
    "print(X.size())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "ll = int(np.ceil(np.log(DD) / np.log(2)))\n",
    "print(ll)\n",
    "LL = 2 ** ll\n",
    "print(LL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "BB = (torch.FloatTensor(LL).uniform_(0, 2).type(torch.LongTensor) * 2 - 1).type(torch.FloatTensor)\n",
    "print(BB.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31,  2, 17,  7,  8, 16, 29, 21, 15,  3,  9, 12, 14,  0, 28, 22, 13, 30,\n",
      "         4, 24,  1, 18, 19, 20, 27, 10, 23, 11,  6, 25, 26,  5])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "Pi = torch.LongTensor(np.random.permutation(LL))\n",
    "print(Pi)\n",
    "print(Pi.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4825, -0.6431, -0.6793, -1.1537,  0.1646,  0.6058,  0.0722, -1.1346,\n",
      "        -1.4484, -0.4230, -0.4068,  1.2528,  1.9447, -1.5819,  0.9514,  0.3535,\n",
      "        -0.2206,  1.6402, -0.8402,  0.2016,  0.1092, -0.6732, -0.1583,  1.0514,\n",
      "         0.7597,  0.9266,  0.7771,  1.2039, -1.6629,  1.5158, -0.8744, -0.0909])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "GG = torch.FloatTensor(LL,).normal_()\n",
    "print(GG)\n",
    "print(GG.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.9100)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "divisor = torch.sqrt(LL * torch.sum(torch.pow(GG, 2)))\n",
    "print(divisor)\n",
    "print(divisor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastfood_vars = [BB, Pi, GG, divisor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6817,  0.9411, -0.4930,  0.9057,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "dd_pad = F.pad(x, pad=(0, LL - dd), value=0, mode='constant')\n",
    "print(dd_pad)\n",
    "print(dd_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6817, -0.9411, -0.4930,  0.9057,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "mul_1 = torch.mul(BB, dd_pad)\n",
    "print(mul_1)\n",
    "print(mul_1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1533, 1.2241, 0.3279, 4.0215, 1.1533, 1.2241, 0.3279, 4.0215, 1.1533,\n",
      "        1.2241, 0.3279, 4.0215, 1.1533, 1.2241, 0.3279, 4.0215, 1.1533, 1.2241,\n",
      "        0.3279, 4.0215, 1.1533, 1.2241, 0.3279, 4.0215, 1.1533, 1.2241, 0.3279,\n",
      "        4.0215, 1.1533, 1.2241, 0.3279, 4.0215])\n",
      "torch.Size([32])\n",
      "[1.1532891  1.2241094  0.32788444 4.021532   1.1532891  1.2241094\n",
      " 0.32788444 4.021532   1.1532891  1.2241094  0.32788444 4.021532\n",
      " 1.1532891  1.2241094  0.32788444 4.021532   1.1532891  1.2241094\n",
      " 0.32788444 4.021532   1.1532891  1.2241094  0.32788444 4.021532\n",
      " 1.1532891  1.2241094  0.32788444 4.021532   1.1532891  1.2241094\n",
      " 0.32788444 4.021532  ]\n"
     ]
    }
   ],
   "source": [
    "mul_2 = fast_walsh_hadamard_torched(mul_1, 0, normalize=False)\n",
    "print(mul_2)\n",
    "print(mul_2.shape)\n",
    "print(mul_2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_3 = mul_2[Pi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_4 = torch.mul(mul_3, GG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_5 = fast_walsh_hadamard_torched(mul_4, 0, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul_5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = torch.div(mul_5[:DD], divisor * np.sqrt(float(DD) / LL ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0953, -0.1891, -0.3242, -0.1385, -0.1784, -0.2170, -0.3925, -0.0953,\n",
       "        -0.5606,  0.1517,  0.8030, -0.2647, -0.4901,  0.2657,  0.6215, -0.3435,\n",
       "        -1.0986,  0.2238, -0.1148, -0.5346])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
